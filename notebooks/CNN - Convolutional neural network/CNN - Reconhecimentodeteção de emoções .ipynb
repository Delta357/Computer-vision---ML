{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3537e8e3",
   "metadata": {
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1627183241238,
     "user": {
      "displayName": "Rafael Henrique Gallo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiTU_KJKA_XDsAysEZ4d3RTCoWXQbom3xYB1t9o564=s64",
      "userId": "12424220659252605848"
     },
     "user_tz": 180
    },
    "id": "3537e8e3"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import zipfile\n",
    "import tensorflow\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c96e7c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1627183257632,
     "user": {
      "displayName": "Rafael Henrique Gallo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiTU_KJKA_XDsAysEZ4d3RTCoWXQbom3xYB1t9o564=s64",
      "userId": "12424220659252605848"
     },
     "user_tz": 180
    },
    "id": "9c96e7c1",
    "outputId": "4fb7a63d-6914-4415-a016-f67ea92eba06"
   },
   "outputs": [],
   "source": [
    "# Versão do tensorflow\n",
    "\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p7yOTKUzrhXG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20382,
     "status": "ok",
     "timestamp": 1627183336059,
     "user": {
      "displayName": "Rafael Henrique Gallo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiTU_KJKA_XDsAysEZ4d3RTCoWXQbom3xYB1t9o564=s64",
      "userId": "12424220659252605848"
     },
     "user_tz": 180
    },
    "id": "p7yOTKUzrhXG",
    "outputId": "56f3c3fa-89e4-4828-f7f0-4151842d3467"
   },
   "outputs": [],
   "source": [
    "# Drive do Google\n",
    "from google.colab import drive\n",
    "\n",
    "colab_drive = drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uxrncIxqrvl2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8008,
     "status": "ok",
     "timestamp": 1627183489975,
     "user": {
      "displayName": "Rafael Henrique Gallo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiTU_KJKA_XDsAysEZ4d3RTCoWXQbom3xYB1t9o564=s64",
      "userId": "12424220659252605848"
     },
     "user_tz": 180
    },
    "id": "uxrncIxqrvl2",
    "outputId": "bf047aff-78d1-40a3-af54-fb2d38184d71"
   },
   "outputs": [],
   "source": [
    "# Dados do drive\n",
    "\n",
    "path = \"/content/gdrive/MyDrive/Cursos/Udemy/Visão Computacional/Projeto VC/Material.zip\"\n",
    "object_zip = zipfile.ZipFile(file = path, mode = \"r\")\n",
    "object_zip.extractall(\"./\")\n",
    "object_zip.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ufpZM2KdtQMz",
   "metadata": {
    "executionInfo": {
     "elapsed": 623,
     "status": "ok",
     "timestamp": 1627183898268,
     "user": {
      "displayName": "Rafael Henrique Gallo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiTU_KJKA_XDsAysEZ4d3RTCoWXQbom3xYB1t9o564=s64",
      "userId": "12424220659252605848"
     },
     "user_tz": 180
    },
    "id": "ufpZM2KdtQMz"
   },
   "outputs": [],
   "source": [
    "# Arquivos dos haarcascade\n",
    "cascade_faces = 'Material/haarcascade_frontalface_default.xml'\n",
    "caminho_modelo = 'Material/modelo_01_expressoes.h5'\n",
    "face_detection = cv2.CascadeClassifier(cascade_faces)\n",
    "classificador_emocoes = load_model(caminho_modelo, compile=False)\n",
    "\n",
    "# Modelo\n",
    "face_detection = cv2.CascadeClassifier(cascade_faces)\n",
    "classificador_emocoes = load_model(caminho_modelo, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kr4InjzmsdVL",
   "metadata": {
    "id": "Kr4InjzmsdVL"
   },
   "source": [
    "### Carregar biblioteca e função\n",
    "\n",
    "Essa parte é uma mistura de python com javascript e HTML. Primeiro é criado no HTML a tag `video` que é onde estará sendo exibido a imagem capturada pela webcam (se quiser visualizar isso em tempo real clique com o botão direito sobre o quadrado onde está exibindo a imagem da webcam e selecione \"Inspecionar\")\n",
    " \n",
    "Em seguida é iniciada a tag `script`, e é dentro dessa tag onde será colocado todo o código javascript, ou seja, javascript é tudo que está entre `<script></script>`\n",
    "\n",
    "A webcam é gerenciada pela função `navigator.mediaDevices.getUserMedia`, que além de gerenciar essa comunicação é responsável por transmitir a imagem recebida pela câmera e adicionar as imagens do frame dentro da tag video.\n",
    "\n",
    "A variável `data` vai receber uma Promise, que no javascript é um objeto usado para processamento assíncrono (esse objeto guarda um valor que pode estar disponível agora, futuramente ou nunca. Isso permite o tratamento de eventos ou ações que acontecem de forma assíncrona (como é nesse caso) em casos de sucessos ou falhas) esse objeto será responsável por fazer o seguinte: \n",
    "ao ser identificado um click sobre a imagem (na tag `<video>`), `video.onclick`\n",
    "pega o frame atual e adiciona ele dentro de um canvas, com a largura e altura especificadas `canvas.getContext('2d').drawImage(video, 0, 0, w, h)`\n",
    "em seguida é retornada as tracks do video (a captura) e é interrompida a captura `video.srcObject.getVideoTracks()[0].stop()`\n",
    "por fim, o conteúdo da tag video é substituído pela imagem capturada, convertando o canvas em imagem (formato 'image/jpeg') para que possa ser exibida. Se ocorreu tudo como deveria, a Promise é concluída ao retornar resolve() como resposta (que contém a imagem)\n",
    "\n",
    "E `</script>` finaliza a tag, pois a partir daqui não adicionaremos mais código em javascript. Toda a string recebida pela variável `VIDEO_HTML` será adicionada ao HTML\n",
    " Para exibir a captura do vídeo na página precisamos fazer isso pois lembre-se que as páginas web utilizam a HTML (linguagem de marcação) para sua construção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6u8GkLdLsd-y",
   "metadata": {
    "executionInfo": {
     "elapsed": 230,
     "status": "ok",
     "timestamp": 1627183526881,
     "user": {
      "displayName": "Rafael Henrique Gallo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiTU_KJKA_XDsAysEZ4d3RTCoWXQbom3xYB1t9o564=s64",
      "userId": "12424220659252605848"
     },
     "user_tz": 180
    },
    "id": "6u8GkLdLsd-y"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Audio\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode\n",
    "import numpy as np\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "VIDEO_HTML = \"\"\"\n",
    "<video autoplay\n",
    " width=%d height=%d style='cursor: pointer;'></video>\n",
    "<script>\n",
    "\n",
    "var video = document.querySelector('video')\n",
    "\n",
    "navigator.mediaDevices.getUserMedia({ video: true })\n",
    "  .then(stream=> video.srcObject = stream)\n",
    "  \n",
    "var data = new Promise(resolve=>{\n",
    "  video.onclick = ()=>{\n",
    "    var canvas = document.createElement('canvas')\n",
    "    var [w,h] = [video.offsetWidth, video.offsetHeight]\n",
    "    canvas.width = w\n",
    "    canvas.height = h\n",
    "    canvas.getContext('2d')\n",
    "          .drawImage(video, 0, 0, w, h)\n",
    "    video.srcObject.getVideoTracks()[0].stop()\n",
    "    video.replaceWith(canvas)\n",
    "    resolve(canvas.toDataURL('image/jpeg', %f))\n",
    "  }\n",
    "})\n",
    "</script>\n",
    "\"\"\"\n",
    "def tirar_foto(filename='photo.jpg', quality=2, size=(400,300)):\n",
    "  display(HTML(VIDEO_HTML % (size[0],size[1],quality)))\n",
    "  data = eval_js(\"data\")\n",
    "  binary = b64decode(data.split(',')[1])\n",
    "  f = io.BytesIO(binary)\n",
    "  return np.asarray(Image.open(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ncd9k4_xsjGG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "executionInfo": {
     "elapsed": 12562,
     "status": "ok",
     "timestamp": 1627183685683,
     "user": {
      "displayName": "Rafael Henrique Gallo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiTU_KJKA_XDsAysEZ4d3RTCoWXQbom3xYB1t9o564=s64",
      "userId": "12424220659252605848"
     },
     "user_tz": 180
    },
    "id": "ncd9k4_xsjGG",
    "outputId": "8eff6e7b-03a8-4ea4-c04d-76b0f20b26a2"
   },
   "outputs": [],
   "source": [
    "# Capturando foto\n",
    "\n",
    "img = tirar_foto() # Clica na imagem para tirar uma foto \n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "cv2_imshow(img)\n",
    "cv2.imwrite(\"Foto-tirada.jpg\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HvbiuhAJtG20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601
    },
    "executionInfo": {
     "elapsed": 31282,
     "status": "ok",
     "timestamp": 1627183972230,
     "user": {
      "displayName": "Rafael Henrique Gallo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiTU_KJKA_XDsAysEZ4d3RTCoWXQbom3xYB1t9o564=s64",
      "userId": "12424220659252605848"
     },
     "user_tz": 180
    },
    "id": "HvbiuhAJtG20",
    "outputId": "eb980f1b-066d-407d-9336-d17c5d2e8ff5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "expressoes = [\"Raiva\", \"Nojo\", \"Medo\", \"Feliz\", \"Triste\", \"Surpreso\", \"Neutro\"]\n",
    " \n",
    "original = img.copy()\n",
    "faces = face_detection.detectMultiScale(original,scaleFactor=1.1,minNeighbors=3,minSize=(20,20))\n",
    "cinza = cv2.cvtColor(original, cv2.COLOR_BGR2GRAY)\n",
    " \n",
    "if len(faces) > 0:\n",
    "    for (fX, fY, fW, fH) in faces:\n",
    "      roi = cinza[fY:fY + fH, fX:fX + fW]\n",
    "      roi = cv2.resize(roi, (48, 48))\n",
    "      roi = roi.astype(\"float\") / 255.0\n",
    "      roi = img_to_array(roi)\n",
    "      roi = np.expand_dims(roi, axis=0)\n",
    "      preds = classificador_emocoes.predict(roi)[0]\n",
    "      print(preds)\n",
    "      emotion_probability = np.max(preds)\n",
    "      label = expressoes[preds.argmax()]\n",
    "      cv2.putText(original, label, (fX, fY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "      cv2.rectangle(original, (fX, fY), (fX + fW, fY + fH),(0, 0, 255), 2)\n",
    "else:\n",
    "    print('Nenhuma face detectada')\n",
    "\n",
    " \n",
    "cv2_imshow(original)\n",
    "\n",
    "probabilidades = np.ones((250, 300, 3), dtype=\"uint8\") * 255\n",
    "# Mostra gráfico apenas se detectou uma face\n",
    "if len(faces) == 1:\n",
    "  for (i, (emotion, prob)) in enumerate(zip(expressoes, preds)):\n",
    "      # Nome das emoções\n",
    "      text = \"{}: {:.2f}%\".format(emotion, prob * 100)\n",
    "      w = int(prob * 300)\n",
    "      cv2.rectangle(probabilidades, (7, (i * 35) + 5),\n",
    "      (w, (i * 35) + 35), (200, 250, 20), -1)\n",
    "      cv2.putText(probabilidades, text, (10, (i * 35) + 23),\n",
    "      cv2.FONT_HERSHEY_SIMPLEX, 0.45,\n",
    "      (0, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "  cv2_imshow(probabilidades)\n",
    "\n",
    "cv2.imwrite(\"captura.jpg\",original)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VAX4XKtiuwT1",
   "metadata": {
    "id": "VAX4XKtiuwT1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CNN - Reconhecimentodeteção de emoções .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
